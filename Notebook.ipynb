{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Loading the data. Preliminary Preprocessing and then saving the data in a Pandas dataframe to have unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Question_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how did serfdom develop in and then leave russ...</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what films featured the character popeye doyle ?</td>\n",
       "      <td>what</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how can i find a list of celebrities ' real na...</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what fowl grabs the spotlight after the chines...</td>\n",
       "      <td>what</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what is the full form of .com ?</td>\n",
       "      <td>what</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question Question_Type\n",
       "0  how did serfdom develop in and then leave russ...       unknown\n",
       "1   what films featured the character popeye doyle ?          what\n",
       "2  how can i find a list of celebrities ' real na...       unknown\n",
       "3  what fowl grabs the spotlight after the chines...          what\n",
       "4                    what is the full form of .com ?          what"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open(\"LabelledData.txt\",\"r\") as text:\n",
    "    lines=text.readlines()\n",
    "lines=[line.replace(\"\\n\",\"\") for line in lines]\n",
    "lines=[line.split(\",,,\") for line in lines]\n",
    "questions=[line[0] for line in lines]\n",
    "question_type=[line[1] for line in lines]\n",
    "question_type=[q.strip() for q in question_type]\n",
    "questions=[q.strip() for q in questions]\n",
    "lines=[[q,w] for (q,w) in zip(questions,question_type)]\n",
    "data=pd.DataFrame(lines,columns=[\"Question\",\"Question_Type\"])\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a set of questions,assigned to a specific question type. The question types are as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['affirmation', 'unknown', 'what', 'when', 'who'], dtype=object), array([104, 272, 609,  96, 402], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "question_types=np.unique(data[\"Question_Type\"].values,return_counts=True)\n",
    "print question_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the classes seems adequately presented, however questions of type \"what\" are over represented in data. Let us visualize it\n",
    "better using a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf/6CGV5RTMoWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe06cda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(list(question_types[0]))),list(question_types[1]))\n",
    "plt.xlabel(\"Question Type\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(range(len(list(question_types[0]))),list(question_types[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set baselines. We will only use one baseline here and then try further models. The baseline is to predict \"What\" for every\n",
    "example. This gives us an accuracy of: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.410654079568\n"
     ]
    }
   ],
   "source": [
    "count=collections.Counter(data[\"Question_Type\"].values)\n",
    "print count[\"what\"]/float(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, we can see without much surprise that we are doing quite a bad job. So it is better to try learning based models. We will\n",
    "build a MaxEnt model for this exercise. But before that,we need to normalize our text data so as to not have redundant representations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_processing(text):\n",
    "    puncts=[',',\"'\",'\"']\n",
    "    stemmer=SnowballStemmer(\"english\")\n",
    "    processed_text=[]\n",
    "    for line in text:\n",
    "        line=line.lower()\n",
    "        line=line.decode('utf-8','ignore')\n",
    "        line=re.sub(r'[0-9]+','',line)\n",
    "        \n",
    "        words=nltk.word_tokenize(line)\n",
    "        words=[w for w in words if w not in puncts]\n",
    "        words=[stemmer.stem(w) for w in words]\n",
    "        processed_text.append(\" \".join(words))\n",
    "     \n",
    "    return processed_text\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbi</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>abil</th>\n",
       "      <th>abolish</th>\n",
       "      <th>abomin</th>\n",
       "      <th>about</th>\n",
       "      <th>academi</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>accessori</th>\n",
       "      <th>...</th>\n",
       "      <th>your</th>\n",
       "      <th>yous</th>\n",
       "      <th>yousuf</th>\n",
       "      <th>zadora</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zenger</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zolotow</th>\n",
       "      <th>zone</th>\n",
       "      <th>zorro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3064 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abbi  abbrevi  abil  abolish  abomin  about  academi  accept  access  \\\n",
       "0   0.0      0.0   0.0      0.0     0.0    0.0      0.0     0.0     0.0   \n",
       "1   0.0      0.0   0.0      0.0     0.0    0.0      0.0     0.0     0.0   \n",
       "2   0.0      0.0   0.0      0.0     0.0    0.0      0.0     0.0     0.0   \n",
       "3   0.0      0.0   0.0      0.0     0.0    0.0      0.0     0.0     0.0   \n",
       "4   0.0      0.0   0.0      0.0     0.0    0.0      0.0     0.0     0.0   \n",
       "\n",
       "   accessori  ...    your  yous  yousuf  zadora  zealand  zenger  zodiac  \\\n",
       "0        0.0  ...     0.0   0.0     0.0     0.0      0.0     0.0     0.0   \n",
       "1        0.0  ...     0.0   0.0     0.0     0.0      0.0     0.0     0.0   \n",
       "2        0.0  ...     0.0   0.0     0.0     0.0      0.0     0.0     0.0   \n",
       "3        0.0  ...     0.0   0.0     0.0     0.0      0.0     0.0     0.0   \n",
       "4        0.0  ...     0.0   0.0     0.0     0.0      0.0     0.0     0.0   \n",
       "\n",
       "   zolotow  zone  zorro  \n",
       "0      0.0   0.0    0.0  \n",
       "1      0.0   0.0    0.0  \n",
       "2      0.0   0.0    0.0  \n",
       "3      0.0   0.0    0.0  \n",
       "4      0.0   0.0    0.0  \n",
       "\n",
       "[5 rows x 3064 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text=data_processing(data[\"Question\"].values)\n",
    "cv=CountVectorizer()\n",
    "tf=TfidfTransformer()\n",
    "\n",
    "cv_data=cv.fit_transform(processed_text)\n",
    "tf_data=tf.fit_transform(cv_data).toarray()\n",
    "tf_df=pd.DataFrame(tf_data,columns=cv.get_feature_names())\n",
    "tf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the feature matrix is very sparse an workin with it is not a really good idea on account of the curse of dimensionality.Let us try to fit a MaxEnt model on this before going further with any other thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_dict={'who':1,\"what\":2,\"when\":3,\"affirmation\":4,\"unknown\":5}\n",
    "y=[label_dict[t] for t in data[\"Question_Type\"].values]\n",
    "Xtrain,Xtest,ytrain,ytest=train_test_split(tf_df.values,y,test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.959595959596\n"
     ]
    }
   ],
   "source": [
    "def train(X,y,algorithm=\"Logistic\"):\n",
    "    if(algorithm==\"Logistic\"):\n",
    "        clf=LogisticRegression(penalty='l2',C=10,multi_class=\"multinomial\",solver=\"newton-cg\")\n",
    "        clf.fit(X,y)\n",
    "    return clf\n",
    "model=train(Xtrain,ytrain)\n",
    "print(model.score(Xtrain,ytrain))\n",
    "print(model.score(Xtest,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, it might seem like we are overfitting on the model but we can't be sure as the test set is really small, so is\n",
    "the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let us write a predicct function to see how we are doing on some data\n",
    "def predict(question,label_dict=label_dict,clf=model,cv=cv,tf=tf):\n",
    "    question=data_processing([question])\n",
    "    \n",
    "    cv_vector=cv.transform(question)\n",
    "    tf_vector=tf.transform(cv_vector).toarray()\n",
    "    pred=clf.predict(tf_vector)\n",
    "    return [key for key in label_dict.keys() if label_dict[key]==pred][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what time does the movie begin? when\n",
      "what is the birthplace of john? what\n",
      "Who was gandhi who\n",
      "Is it morning or noon? affirmation\n",
      "are you a smuggler? affirmation\n",
      "what time does the train leave? when\n",
      "what is the time? what\n"
     ]
    }
   ],
   "source": [
    "#Some testing sentences:\n",
    "sents=[\"what time does the movie begin?\",\"what is the birthplace of john?\",\"Who was gandhi\",\"Is it morning or noon?\",\n",
    "      \"are you a smuggler?\",\"what time does the train leave?\",\"what is the time?\"]\n",
    "for sent in sents:\n",
    "    print sent,predict(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They seem to be working well, let us use a larger testing corpus. Since our dataset was approximately a 1000 sentences, lets \n",
    "sample 200 sentences from  http://cogcomp.cs.illinois.edu/Data/QA/QC/train_1000.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I get some free technical information on Electric Vehicle ? unknown DESC:manner\n",
      "What is the daily requirement of folic acid for an expectant mother ? what DESC:desc\n",
      "What was American folk hero John Chapman 's nickname ? what HUM:ind\n",
      "What is a female rabbit called ? what ENTY:animal\n",
      "What was the U.S. highway death toll in 1969 ? what NUM:count\n",
      "What Nevada center has been dubbed The Biggest Little City in the World ? what LOC:city\n",
      "When did Mount St. Helen last have a significant eruption ? when NUM:date\n",
      "What Spanish artist painted Crucifixion ? what HUM:ind\n",
      "How many species of sharks are there ? unknown NUM:count\n",
      "Where on the Internet can I get information about the Fifth Amendment on the American Bill of Rights ? unknown LOC:other\n",
      "Who coined the term NN cyberspace `` in his novel NN Neuromancer '' ? who HUM:ind\n"
     ]
    }
   ],
   "source": [
    "#Loading the testing data:\n",
    "with open('train_1000.label.txt','r') as filename:\n",
    "    lines=filename.readlines()\n",
    "test_labels=[]\n",
    "test_sents=[]\n",
    "for line in lines:\n",
    "    line=line.lstrip()\n",
    "    line=line.rstrip()\n",
    "    line=line.replace(\"\\n\",\"\")\n",
    "    words=line.split()\n",
    "   \n",
    "    test_labels.append(words[0])\n",
    "    test_sents.append(\" \".join(words[1:]))\n",
    "\n",
    "#Now lets randomly sample 200 sentences from the dataset:\n",
    "test_indices=np.random.permutation(len(test_sents))[:200]\n",
    "\n",
    "labels=[test_labels[ind] for ind in test_indices]\n",
    "sents=[test_sents[ind] for ind in test_indices]\n",
    "\n",
    "#Lets print predictions for first ten sentences:\n",
    "for i,sent in enumerate(sents):\n",
    "    print sent,predict(sent),labels[i]\n",
    "    if(i==10):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the predictions, that we are doing reasonably well in our predictions. The l2 regularizaion is penalizing \n",
    "terms so we don't really need to subsample features, as it will probably degrade performance than improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'LabelledData.txt',\n",
       " 'Notebook.ipynb',\n",
       " 'QuestionClassifier.py',\n",
       " 'train_1000.label.txt']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
